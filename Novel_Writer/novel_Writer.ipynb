{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word embedding look up loading\n",
    "word2vec_lookup = np.load('word2vec_gensim.npy')\n",
    "word2vec_lookup = word2vec_lookup[None][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "words2oneHot = dict()\n",
    "words = np.array(list(word2vec_lookup.keys()))\n",
    "voca_size = len(words)\n",
    "for i in range(voca_size):\n",
    "    words2oneHot[words[i]] = np.zeros(voca_size).astype('int')\n",
    "    words2oneHot[words[i]][i] = 1\n",
    "oneHotPos2vec_lookup = dict()\n",
    "oneHotPos2words = dict()\n",
    "for key, val in words2oneHot.items():\n",
    "    oneHotPos2vec_lookup[np.argmax(words2oneHot[key])] = word2vec_lookup[key]\n",
    "    oneHotPos2words[np.argmax(words2oneHot[key])] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_list = [list(oneHotPos2vec_lookup[i]) for i in range(len(oneHotPos2vec_lookup.keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /media/warrents/6A085EA7085E7255/GoogleDriveSync/WarrenTS/Code/Code/Python2.7/IPython_Notebooks/MachineLearning/pecu_course/Text_mining_LSTM/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u86847c3b15d467aa53d8311fcf6be389.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.172 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word segmentation done. len = 19416 2207\n"
     ]
    }
   ],
   "source": [
    "# romance condor heroes vectors\n",
    "with open('romance_condor_heroes.txt', 'r') as f:\n",
    "    data = list(f)\n",
    "sentences = list()\n",
    "for i in range(len(data)):\n",
    "    sentences += data[i].replace(' ', '').split('ã€‚')\n",
    "print('Pre-processing done.')\n",
    "sent_seg = list()\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "sent_seg_too_long = list()\n",
    "for i in range(len(sentences)):\n",
    "    s = sentences[i]\n",
    "    s = list(jieba.cut(s))\n",
    "    if len(s) > 50:\n",
    "        sent_seg_too_long.append(s)\n",
    "    else:\n",
    "        unknowns = ['<unknown>' for j in range(50-len(s))]\n",
    "        s += unknowns\n",
    "        sent_seg.append(s)\n",
    "sent_seg = np.array(sent_seg)\n",
    "print('word segmentation done.', 'len =', len(sent_seg), len(sent_seg_too_long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Batch data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_X(i, batch):\n",
    "    batch_x = list()\n",
    "    for s in sent_seg[i:i+batch]:\n",
    "        v = list()\n",
    "        for w in s:\n",
    "            if w in word2vec_lookup:\n",
    "                v.append(word2vec_lookup[w])\n",
    "            else:\n",
    "                v.append(word2vec_lookup['<unknown>'])\n",
    "        batch_x.append(v)\n",
    "    batch_x = np.array(batch_x)\n",
    "    return batch_x\n",
    "def batch_Y(i, batch):\n",
    "    batch_y = list()\n",
    "    for s in sent_seg[i+1:i+batch+1]:\n",
    "        v = list()\n",
    "        for w in s:\n",
    "            if w in words2oneHot:\n",
    "                v.append(words2oneHot[w])\n",
    "            else:\n",
    "                v.append(words2oneHot['<unknown>'])\n",
    "        batch_y.append(v)\n",
    "    batch_y = np.array(batch_y)\n",
    "    return batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 50, 64), array([[-2.12412047,  1.26558292,  0.48184228, ..., -1.78398168,\n",
       "         -0.11021218, -0.40292478],\n",
       "        [ 0.24595512, -0.22340411,  0.46395406, ..., -0.27114129,\n",
       "         -0.14347288, -0.52333629],\n",
       "        [ 0.70083529, -0.13018842,  1.39388931, ..., -0.0787876 ,\n",
       "          0.99347192, -0.10601542],\n",
       "        ..., \n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]], dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x = batch_X(0, 50)\n",
    "batch_y = batch_Y(0, 50)\n",
    "batch_x.shape, batch_x[45] # (batch size, sent len, vec len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## tf.embedding_lookup test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[10 20 30 40]\n",
      "  [10 20 30 40]\n",
      "  [10 20 30 40]\n",
      "  [40 30 20 50]]\n",
      "\n",
      " [[10 20 30 40]\n",
      "  [10 20 30 40]\n",
      "  [40 30 20 50]\n",
      "  [10 20 30 40]]]\n"
     ]
    }
   ],
   "source": [
    "params = tf.constant([[10,20,30,40], [40,30,20,50]])\n",
    "ids1 = tf.constant([[0,0,0,1], [0,0,1,0]])\n",
    "#ids2 = tf.constant([0,3,3,2,3,1,2])\n",
    "with tf.Session() as sess:\n",
    "    tt = sess.run(tf.nn.embedding_lookup(params,ids1))\n",
    "    print(tt)\n",
    "    #print(sess.run(tf.nn.embedding_lookup(params,ids2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "params2 = tf.constant(vec_list)\n",
    "\n",
    "ids3 = tf.constant([0, 1])\n",
    "ids4 = tf.arg_max(tf.constant([[1, 0, 0], [0, 1, 0]]), dimension=1)\n",
    "with tf.Session() as sess:\n",
    "    _ = sess.run(tf.nn.embedding_lookup(params2,ids3))\n",
    "    _2 = sess.run(tf.nn.embedding_lookup(params2,ids4))\n",
    "    print(sess.run(ids4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_size = len(word2vec_lookup['<go>'])\n",
    "oneHot_size = len(words2oneHot['<go>'])\n",
    "enc_len = 50\n",
    "dec_len = enc_len\n",
    "n_layer1 = 256\n",
    "l_r = 1e-3\n",
    "epoch = 3\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Weight and bias functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate random weight and bias\n",
    "def _weight(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def _bias(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Encoder and Decoder input\n",
    " <br> 50:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word vectors constant\n",
    "vecs = tf.constant(vec_list)\n",
    "\n",
    "# input - word vectors\n",
    "word_vec = [tf.placeholder(tf.float32, shape=[None, vec_size], name='word_vec') for i in range(enc_len)]    \n",
    "\n",
    "# encoder input\n",
    "w_enc = _weight([vec_size, n_layer1])\n",
    "b_enc = _bias([n_layer1])\n",
    "enc_inp = [tf.nn.dropout(tf.nn.relu(tf.matmul(word_vec[i], w_enc)+b_enc), keep_prob=.8, name='enc_inp') \\\n",
    "           for i in range(enc_len)]\n",
    "\n",
    "# training target - word one-hot\n",
    "word_target = [tf.placeholder(tf.int32, shape=[None, oneHot_size], name='word_target') for i in range(dec_len)]\n",
    "\n",
    "# one-hot to word vectors\n",
    "target = [tf.nn.embedding_lookup(vecs,tf.arg_max(word_target[i], dimension=1)) for i in range(dec_len-1)]\n",
    "target = [tf.zeros_like(target[0], dtype=np.float32, name=\"GO\")] + target\n",
    "\n",
    "# decoder input\n",
    "w_dec = _weight([vec_size, n_layer1])\n",
    "b_dec = _bias([n_layer1])\n",
    "dec_inp = [tf.nn.dropout(tf.nn.relu(tf.matmul(target[i], w_dec)+b_dec), keep_prob=.8, name='dec_inp') \\\n",
    "           for i in range(dec_len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hiddens = 2\n",
    "cells = [tf.contrib.rnn.BasicLSTMCell(n_layer1, forget_bias=1.0) for i in range(n_hiddens)]\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "dec_outputs, dec_memory = tf.contrib.legacy_seq2seq.basic_rnn_seq2seq(\n",
    "        enc_inp, \n",
    "        dec_inp, \n",
    "        cell\n",
    "    )\n",
    "\n",
    "w_dec_o = _weight([n_layer1, oneHot_size])\n",
    "b_dec_o = _weight([oneHot_size])\n",
    "dec_outputs = [tf.nn.softmax(tf.matmul(dec_outputs[i], w_dec_o)+b_dec_o, name='dec_out') \\\n",
    "               for i in range(dec_len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "accu = 0\n",
    "for y_hat, y_real in zip(dec_outputs, word_target):\n",
    "    loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat, labels=y_real))\n",
    "    accu += tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_hat, 1), tf.argmax(y_real, 1)), tf.float32))\n",
    "\n",
    "op = tf.train.RMSPropOptimizer(l_r).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - ***Training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "464.766 0.0\n",
      "100\n",
      "464.766 0.0\n",
      "150\n",
      "432.642 32.14\n",
      "200\n",
      "432.035 32.74\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "#with tf.Session() as sess:\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for epo in range(epoch):\n",
    "    # len(sent_seg) == 19416\n",
    "    for i in range(int(len(sent_seg)/batch_size)):\n",
    "        batch_x = batch_X(i*batch_size, batch_size)\n",
    "        batch_y = batch_Y(i*batch_size, batch_size)\n",
    "        feed_dict = {word_vec[t]: batch_x[:, t] for t in range(enc_len)}\n",
    "        feed_dict.update({word_target[t]: batch_y[:, t] for t in range(dec_len)})\n",
    "        _ = sess.run([op], feed_dict=feed_dict)\n",
    "        lo, ac = sess.run([loss, accu], feed_dict=feed_dict)\n",
    "        if (i+1)%50 == 0:\n",
    "            print(i+1)\n",
    "            lo, ac = sess.run([loss, accu], feed_dict=feed_dict)\n",
    "            print(lo, ac)\n",
    "        #if (epo+1)%10 == 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words2oneHot\n",
    "word2vec_lookup\n",
    "oneHotPos2words\n",
    "oneHotPos2vec_lookup\n",
    "sent_seg\n",
    "sent_seg_too_long"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
